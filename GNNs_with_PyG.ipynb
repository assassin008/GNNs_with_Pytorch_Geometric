{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install torch-geometric\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rDbaNNgebuM",
        "outputId": "5cda9be9-e212-4b43-c9cc-ec81d221b029"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m803.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.5.2-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.4.0)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data"
      ],
      "metadata": {
        "id": "IFURsQdiebwy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import Planetoid\n",
        "\n",
        "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n"
      ],
      "metadata": {
        "id": "8aeZDa6Reb3j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ce9d1ee-640c-48cb-c536-210f64e588ca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, 16)\n",
        "        self.conv2 = GCNConv(16, out_channels)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ],
      "metadata": {
        "id": "8hneIunUeb7A"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GCN(dataset.num_node_features, dataset.num_classes).to(device)\n",
        "data = dataset[0].to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(200):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n"
      ],
      "metadata": {
        "id": "4Vc9a-i3eb-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3df83027-2a94-45fc-ca31-d97a10a35f79"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.948481559753418\n",
            "Epoch 2, Loss: 1.862858772277832\n",
            "Epoch 3, Loss: 1.7502641677856445\n",
            "Epoch 4, Loss: 1.6401373147964478\n",
            "Epoch 5, Loss: 1.5111979246139526\n",
            "Epoch 6, Loss: 1.3893908262252808\n",
            "Epoch 7, Loss: 1.2914828062057495\n",
            "Epoch 8, Loss: 1.1358057260513306\n",
            "Epoch 9, Loss: 1.0419921875\n",
            "Epoch 10, Loss: 0.8974969983100891\n",
            "Epoch 11, Loss: 0.8345098495483398\n",
            "Epoch 12, Loss: 0.7416336536407471\n",
            "Epoch 13, Loss: 0.6646862626075745\n",
            "Epoch 14, Loss: 0.5565491914749146\n",
            "Epoch 15, Loss: 0.5173380970954895\n",
            "Epoch 16, Loss: 0.4519515931606293\n",
            "Epoch 17, Loss: 0.38276341557502747\n",
            "Epoch 18, Loss: 0.33981502056121826\n",
            "Epoch 19, Loss: 0.2992097735404968\n",
            "Epoch 20, Loss: 0.29955118894577026\n",
            "Epoch 21, Loss: 0.31198781728744507\n",
            "Epoch 22, Loss: 0.2114304155111313\n",
            "Epoch 23, Loss: 0.19332531094551086\n",
            "Epoch 24, Loss: 0.16351166367530823\n",
            "Epoch 25, Loss: 0.16651928424835205\n",
            "Epoch 26, Loss: 0.15282106399536133\n",
            "Epoch 27, Loss: 0.13675978779792786\n",
            "Epoch 28, Loss: 0.12937752902507782\n",
            "Epoch 29, Loss: 0.11955070495605469\n",
            "Epoch 30, Loss: 0.12406004220247269\n",
            "Epoch 31, Loss: 0.10551577806472778\n",
            "Epoch 32, Loss: 0.09098613262176514\n",
            "Epoch 33, Loss: 0.08628953993320465\n",
            "Epoch 34, Loss: 0.0996757224202156\n",
            "Epoch 35, Loss: 0.10298462957143784\n",
            "Epoch 36, Loss: 0.06392434984445572\n",
            "Epoch 37, Loss: 0.07503051310777664\n",
            "Epoch 38, Loss: 0.10665538907051086\n",
            "Epoch 39, Loss: 0.051893334835767746\n",
            "Epoch 40, Loss: 0.07649681717157364\n",
            "Epoch 41, Loss: 0.07540181279182434\n",
            "Epoch 42, Loss: 0.06334046274423599\n",
            "Epoch 43, Loss: 0.05721360072493553\n",
            "Epoch 44, Loss: 0.06401452422142029\n",
            "Epoch 45, Loss: 0.0548921562731266\n",
            "Epoch 46, Loss: 0.04855269938707352\n",
            "Epoch 47, Loss: 0.039768755435943604\n",
            "Epoch 48, Loss: 0.07446577399969101\n",
            "Epoch 49, Loss: 0.05289563164114952\n",
            "Epoch 50, Loss: 0.03583582863211632\n",
            "Epoch 51, Loss: 0.04275073483586311\n",
            "Epoch 52, Loss: 0.046385928988456726\n",
            "Epoch 53, Loss: 0.08182667195796967\n",
            "Epoch 54, Loss: 0.04369240999221802\n",
            "Epoch 55, Loss: 0.05776270478963852\n",
            "Epoch 56, Loss: 0.04755842313170433\n",
            "Epoch 57, Loss: 0.030221717432141304\n",
            "Epoch 58, Loss: 0.05063702166080475\n",
            "Epoch 59, Loss: 0.03483004495501518\n",
            "Epoch 60, Loss: 0.0484408438205719\n",
            "Epoch 61, Loss: 0.029386067762970924\n",
            "Epoch 62, Loss: 0.045440517365932465\n",
            "Epoch 63, Loss: 0.05091682821512222\n",
            "Epoch 64, Loss: 0.054256368428468704\n",
            "Epoch 65, Loss: 0.04206804558634758\n",
            "Epoch 66, Loss: 0.04500436410307884\n",
            "Epoch 67, Loss: 0.04913858696818352\n",
            "Epoch 68, Loss: 0.045903678983449936\n",
            "Epoch 69, Loss: 0.0324472151696682\n",
            "Epoch 70, Loss: 0.038324277848005295\n",
            "Epoch 71, Loss: 0.035565078258514404\n",
            "Epoch 72, Loss: 0.0626099556684494\n",
            "Epoch 73, Loss: 0.05026843026280403\n",
            "Epoch 74, Loss: 0.03898647055029869\n",
            "Epoch 75, Loss: 0.045762259513139725\n",
            "Epoch 76, Loss: 0.038331806659698486\n",
            "Epoch 77, Loss: 0.038161296397447586\n",
            "Epoch 78, Loss: 0.03782421723008156\n",
            "Epoch 79, Loss: 0.04984072968363762\n",
            "Epoch 80, Loss: 0.034111905843019485\n",
            "Epoch 81, Loss: 0.044344376772642136\n",
            "Epoch 82, Loss: 0.03752101585268974\n",
            "Epoch 83, Loss: 0.03157331421971321\n",
            "Epoch 84, Loss: 0.03191891312599182\n",
            "Epoch 85, Loss: 0.03480369970202446\n",
            "Epoch 86, Loss: 0.04576878622174263\n",
            "Epoch 87, Loss: 0.03707106038928032\n",
            "Epoch 88, Loss: 0.04611653462052345\n",
            "Epoch 89, Loss: 0.03539489582180977\n",
            "Epoch 90, Loss: 0.04112528637051582\n",
            "Epoch 91, Loss: 0.05128401517868042\n",
            "Epoch 92, Loss: 0.03167641907930374\n",
            "Epoch 93, Loss: 0.031466055661439896\n",
            "Epoch 94, Loss: 0.026884514838457108\n",
            "Epoch 95, Loss: 0.03932769224047661\n",
            "Epoch 96, Loss: 0.03454840928316116\n",
            "Epoch 97, Loss: 0.03404051065444946\n",
            "Epoch 98, Loss: 0.04108455032110214\n",
            "Epoch 99, Loss: 0.04352102056145668\n",
            "Epoch 100, Loss: 0.03687462583184242\n",
            "Epoch 101, Loss: 0.03494962304830551\n",
            "Epoch 102, Loss: 0.048538751900196075\n",
            "Epoch 103, Loss: 0.05170131474733353\n",
            "Epoch 104, Loss: 0.042844537645578384\n",
            "Epoch 105, Loss: 0.030030053108930588\n",
            "Epoch 106, Loss: 0.03993654251098633\n",
            "Epoch 107, Loss: 0.03323765844106674\n",
            "Epoch 108, Loss: 0.03636263310909271\n",
            "Epoch 109, Loss: 0.041034996509552\n",
            "Epoch 110, Loss: 0.0385470986366272\n",
            "Epoch 111, Loss: 0.042514145374298096\n",
            "Epoch 112, Loss: 0.03886166214942932\n",
            "Epoch 113, Loss: 0.058658625930547714\n",
            "Epoch 114, Loss: 0.041470903903245926\n",
            "Epoch 115, Loss: 0.040207743644714355\n",
            "Epoch 116, Loss: 0.03575310856103897\n",
            "Epoch 117, Loss: 0.034646689891815186\n",
            "Epoch 118, Loss: 0.03796251863241196\n",
            "Epoch 119, Loss: 0.035352639853954315\n",
            "Epoch 120, Loss: 0.027398105710744858\n",
            "Epoch 121, Loss: 0.05130070447921753\n",
            "Epoch 122, Loss: 0.046428367495536804\n",
            "Epoch 123, Loss: 0.027647757902741432\n",
            "Epoch 124, Loss: 0.037996698170900345\n",
            "Epoch 125, Loss: 0.026603827252984047\n",
            "Epoch 126, Loss: 0.04193706437945366\n",
            "Epoch 127, Loss: 0.031802766025066376\n",
            "Epoch 128, Loss: 0.036325566470623016\n",
            "Epoch 129, Loss: 0.023074403405189514\n",
            "Epoch 130, Loss: 0.04571957886219025\n",
            "Epoch 131, Loss: 0.02067335695028305\n",
            "Epoch 132, Loss: 0.023040926083922386\n",
            "Epoch 133, Loss: 0.0264086052775383\n",
            "Epoch 134, Loss: 0.03253142908215523\n",
            "Epoch 135, Loss: 0.02500266022980213\n",
            "Epoch 136, Loss: 0.03897474333643913\n",
            "Epoch 137, Loss: 0.034000057727098465\n",
            "Epoch 138, Loss: 0.03900504484772682\n",
            "Epoch 139, Loss: 0.03408452868461609\n",
            "Epoch 140, Loss: 0.04035869985818863\n",
            "Epoch 141, Loss: 0.025245357304811478\n",
            "Epoch 142, Loss: 0.02508939988911152\n",
            "Epoch 143, Loss: 0.026214292272925377\n",
            "Epoch 144, Loss: 0.04116632416844368\n",
            "Epoch 145, Loss: 0.03147049993276596\n",
            "Epoch 146, Loss: 0.024998148903250694\n",
            "Epoch 147, Loss: 0.03487043082714081\n",
            "Epoch 148, Loss: 0.029989924281835556\n",
            "Epoch 149, Loss: 0.03208683431148529\n",
            "Epoch 150, Loss: 0.020370712503790855\n",
            "Epoch 151, Loss: 0.025784799829125404\n",
            "Epoch 152, Loss: 0.03057277947664261\n",
            "Epoch 153, Loss: 0.04408886283636093\n",
            "Epoch 154, Loss: 0.031215770170092583\n",
            "Epoch 155, Loss: 0.02615293115377426\n",
            "Epoch 156, Loss: 0.03800922632217407\n",
            "Epoch 157, Loss: 0.02534445933997631\n",
            "Epoch 158, Loss: 0.030323052778840065\n",
            "Epoch 159, Loss: 0.02608376182615757\n",
            "Epoch 160, Loss: 0.028506183996796608\n",
            "Epoch 161, Loss: 0.036232102662324905\n",
            "Epoch 162, Loss: 0.04250023141503334\n",
            "Epoch 163, Loss: 0.03633677214384079\n",
            "Epoch 164, Loss: 0.026563379913568497\n",
            "Epoch 165, Loss: 0.02168400213122368\n",
            "Epoch 166, Loss: 0.02736501395702362\n",
            "Epoch 167, Loss: 0.04176934435963631\n",
            "Epoch 168, Loss: 0.022107653319835663\n",
            "Epoch 169, Loss: 0.020421521738171577\n",
            "Epoch 170, Loss: 0.02281659096479416\n",
            "Epoch 171, Loss: 0.016598770394921303\n",
            "Epoch 172, Loss: 0.02763218991458416\n",
            "Epoch 173, Loss: 0.0345626026391983\n",
            "Epoch 174, Loss: 0.021661978214979172\n",
            "Epoch 175, Loss: 0.019888708367943764\n",
            "Epoch 176, Loss: 0.029263168573379517\n",
            "Epoch 177, Loss: 0.030781874433159828\n",
            "Epoch 178, Loss: 0.019748669117689133\n",
            "Epoch 179, Loss: 0.024511022493243217\n",
            "Epoch 180, Loss: 0.0492335744202137\n",
            "Epoch 181, Loss: 0.03685009852051735\n",
            "Epoch 182, Loss: 0.02944083884358406\n",
            "Epoch 183, Loss: 0.04427016153931618\n",
            "Epoch 184, Loss: 0.03791569545865059\n",
            "Epoch 185, Loss: 0.023928707465529442\n",
            "Epoch 186, Loss: 0.035850487649440765\n",
            "Epoch 187, Loss: 0.028371108695864677\n",
            "Epoch 188, Loss: 0.02526363544166088\n",
            "Epoch 189, Loss: 0.03145657107234001\n",
            "Epoch 190, Loss: 0.03834813833236694\n",
            "Epoch 191, Loss: 0.02462012879550457\n",
            "Epoch 192, Loss: 0.02418796345591545\n",
            "Epoch 193, Loss: 0.03389115259051323\n",
            "Epoch 194, Loss: 0.021519234403967857\n",
            "Epoch 195, Loss: 0.03746847063302994\n",
            "Epoch 196, Loss: 0.025636382400989532\n",
            "Epoch 197, Loss: 0.01722823642194271\n",
            "Epoch 198, Loss: 0.020791852846741676\n",
            "Epoch 199, Loss: 0.03229987993836403\n",
            "Epoch 200, Loss: 0.022851750254631042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "_, pred = model(data).max(dim=1)\n",
        "correct = int(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
        "accuracy = correct / int(data.test_mask.sum())\n",
        "print(f'Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "id": "ZuNx2anrecBm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92257490-7c95-4faf-aa45-e6b4b2971e30"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GATConv\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GAT, self).__init__()\n",
        "        self.conv1 = GATConv(in_channels, 8, heads=8, dropout=0.6)\n",
        "        self.conv2 = GATConv(8 * 8, out_channels, heads=1, concat=False, dropout=0.6)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = F.elu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ],
      "metadata": {
        "id": "PDfTPhQdecG2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GAT(dataset.num_node_features, dataset.num_classes).to(device)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(200):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n"
      ],
      "metadata": {
        "id": "UY59ZQyEeGIi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4d6e364-1412-4597-f8ca-c5ac9969a761"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.053623676300049\n",
            "Epoch 2, Loss: 2.0357770919799805\n",
            "Epoch 3, Loss: 1.995967149734497\n",
            "Epoch 4, Loss: 2.018946409225464\n",
            "Epoch 5, Loss: 2.021136999130249\n",
            "Epoch 6, Loss: 2.004504442214966\n",
            "Epoch 7, Loss: 1.9476691484451294\n",
            "Epoch 8, Loss: 2.022987127304077\n",
            "Epoch 9, Loss: 2.0058791637420654\n",
            "Epoch 10, Loss: 2.0157382488250732\n",
            "Epoch 11, Loss: 2.0013160705566406\n",
            "Epoch 12, Loss: 2.078397750854492\n",
            "Epoch 13, Loss: 1.9793949127197266\n",
            "Epoch 14, Loss: 1.9922879934310913\n",
            "Epoch 15, Loss: 2.034505844116211\n",
            "Epoch 16, Loss: 1.975099802017212\n",
            "Epoch 17, Loss: 2.019556760787964\n",
            "Epoch 18, Loss: 1.9973750114440918\n",
            "Epoch 19, Loss: 1.9941303730010986\n",
            "Epoch 20, Loss: 2.0310449600219727\n",
            "Epoch 21, Loss: 1.9679110050201416\n",
            "Epoch 22, Loss: 1.984559416770935\n",
            "Epoch 23, Loss: 1.968863606452942\n",
            "Epoch 24, Loss: 2.0748863220214844\n",
            "Epoch 25, Loss: 2.1042873859405518\n",
            "Epoch 26, Loss: 1.9710577726364136\n",
            "Epoch 27, Loss: 1.9721705913543701\n",
            "Epoch 28, Loss: 1.9819270372390747\n",
            "Epoch 29, Loss: 2.015488624572754\n",
            "Epoch 30, Loss: 2.0008127689361572\n",
            "Epoch 31, Loss: 2.0114593505859375\n",
            "Epoch 32, Loss: 1.9981715679168701\n",
            "Epoch 33, Loss: 1.987665057182312\n",
            "Epoch 34, Loss: 2.052227258682251\n",
            "Epoch 35, Loss: 2.024073600769043\n",
            "Epoch 36, Loss: 1.9760890007019043\n",
            "Epoch 37, Loss: 1.975831151008606\n",
            "Epoch 38, Loss: 1.9992517232894897\n",
            "Epoch 39, Loss: 2.01265025138855\n",
            "Epoch 40, Loss: 2.02477765083313\n",
            "Epoch 41, Loss: 2.034170627593994\n",
            "Epoch 42, Loss: 1.9912580251693726\n",
            "Epoch 43, Loss: 2.049459934234619\n",
            "Epoch 44, Loss: 2.0377862453460693\n",
            "Epoch 45, Loss: 1.9876028299331665\n",
            "Epoch 46, Loss: 1.9879810810089111\n",
            "Epoch 47, Loss: 2.0324437618255615\n",
            "Epoch 48, Loss: 1.9903130531311035\n",
            "Epoch 49, Loss: 2.0523269176483154\n",
            "Epoch 50, Loss: 2.0061910152435303\n",
            "Epoch 51, Loss: 2.0090091228485107\n",
            "Epoch 52, Loss: 1.960017204284668\n",
            "Epoch 53, Loss: 2.029759645462036\n",
            "Epoch 54, Loss: 1.956483006477356\n",
            "Epoch 55, Loss: 2.0223183631896973\n",
            "Epoch 56, Loss: 2.022681951522827\n",
            "Epoch 57, Loss: 1.998617172241211\n",
            "Epoch 58, Loss: 1.9937855005264282\n",
            "Epoch 59, Loss: 2.015232563018799\n",
            "Epoch 60, Loss: 2.0292105674743652\n",
            "Epoch 61, Loss: 2.0376741886138916\n",
            "Epoch 62, Loss: 1.9668775796890259\n",
            "Epoch 63, Loss: 2.000898838043213\n",
            "Epoch 64, Loss: 2.007829189300537\n",
            "Epoch 65, Loss: 1.9727953672409058\n",
            "Epoch 66, Loss: 1.9713753461837769\n",
            "Epoch 67, Loss: 2.0178823471069336\n",
            "Epoch 68, Loss: 1.9882644414901733\n",
            "Epoch 69, Loss: 1.993166446685791\n",
            "Epoch 70, Loss: 2.016017198562622\n",
            "Epoch 71, Loss: 2.0122287273406982\n",
            "Epoch 72, Loss: 1.976052165031433\n",
            "Epoch 73, Loss: 2.021564483642578\n",
            "Epoch 74, Loss: 1.960516333580017\n",
            "Epoch 75, Loss: 2.026874542236328\n",
            "Epoch 76, Loss: 1.9965320825576782\n",
            "Epoch 77, Loss: 2.0002219676971436\n",
            "Epoch 78, Loss: 2.0196568965911865\n",
            "Epoch 79, Loss: 2.054189682006836\n",
            "Epoch 80, Loss: 1.9892164468765259\n",
            "Epoch 81, Loss: 2.022181749343872\n",
            "Epoch 82, Loss: 2.0370097160339355\n",
            "Epoch 83, Loss: 1.973511815071106\n",
            "Epoch 84, Loss: 1.981353759765625\n",
            "Epoch 85, Loss: 1.9923503398895264\n",
            "Epoch 86, Loss: 1.9864460229873657\n",
            "Epoch 87, Loss: 2.0097787380218506\n",
            "Epoch 88, Loss: 1.9931291341781616\n",
            "Epoch 89, Loss: 2.0165460109710693\n",
            "Epoch 90, Loss: 1.9870991706848145\n",
            "Epoch 91, Loss: 1.9999773502349854\n",
            "Epoch 92, Loss: 2.022665500640869\n",
            "Epoch 93, Loss: 2.018319845199585\n",
            "Epoch 94, Loss: 2.001037359237671\n",
            "Epoch 95, Loss: 1.954978346824646\n",
            "Epoch 96, Loss: 2.0117335319519043\n",
            "Epoch 97, Loss: 1.9644088745117188\n",
            "Epoch 98, Loss: 1.9924442768096924\n",
            "Epoch 99, Loss: 2.0169131755828857\n",
            "Epoch 100, Loss: 1.9693306684494019\n",
            "Epoch 101, Loss: 2.0085856914520264\n",
            "Epoch 102, Loss: 2.0143954753875732\n",
            "Epoch 103, Loss: 1.9488157033920288\n",
            "Epoch 104, Loss: 2.067111015319824\n",
            "Epoch 105, Loss: 1.9972937107086182\n",
            "Epoch 106, Loss: 2.053107738494873\n",
            "Epoch 107, Loss: 1.9856523275375366\n",
            "Epoch 108, Loss: 2.0074050426483154\n",
            "Epoch 109, Loss: 2.008805513381958\n",
            "Epoch 110, Loss: 1.958309292793274\n",
            "Epoch 111, Loss: 1.9779592752456665\n",
            "Epoch 112, Loss: 2.052520751953125\n",
            "Epoch 113, Loss: 1.9989970922470093\n",
            "Epoch 114, Loss: 2.0016252994537354\n",
            "Epoch 115, Loss: 1.9889953136444092\n",
            "Epoch 116, Loss: 2.003847122192383\n",
            "Epoch 117, Loss: 1.9911869764328003\n",
            "Epoch 118, Loss: 2.013450860977173\n",
            "Epoch 119, Loss: 2.020026683807373\n",
            "Epoch 120, Loss: 2.055830240249634\n",
            "Epoch 121, Loss: 2.0367729663848877\n",
            "Epoch 122, Loss: 1.9876776933670044\n",
            "Epoch 123, Loss: 2.0496833324432373\n",
            "Epoch 124, Loss: 1.9772284030914307\n",
            "Epoch 125, Loss: 2.0470988750457764\n",
            "Epoch 126, Loss: 2.0100913047790527\n",
            "Epoch 127, Loss: 2.0154151916503906\n",
            "Epoch 128, Loss: 1.9508100748062134\n",
            "Epoch 129, Loss: 1.9385114908218384\n",
            "Epoch 130, Loss: 1.9902859926223755\n",
            "Epoch 131, Loss: 2.006186008453369\n",
            "Epoch 132, Loss: 2.0174620151519775\n",
            "Epoch 133, Loss: 2.0405070781707764\n",
            "Epoch 134, Loss: 2.0383121967315674\n",
            "Epoch 135, Loss: 1.9688899517059326\n",
            "Epoch 136, Loss: 2.026925563812256\n",
            "Epoch 137, Loss: 1.9930064678192139\n",
            "Epoch 138, Loss: 2.0100457668304443\n",
            "Epoch 139, Loss: 2.044301748275757\n",
            "Epoch 140, Loss: 1.9665943384170532\n",
            "Epoch 141, Loss: 2.013810634613037\n",
            "Epoch 142, Loss: 2.0243618488311768\n",
            "Epoch 143, Loss: 2.0211517810821533\n",
            "Epoch 144, Loss: 1.981992244720459\n",
            "Epoch 145, Loss: 2.02390193939209\n",
            "Epoch 146, Loss: 1.9935357570648193\n",
            "Epoch 147, Loss: 2.0240142345428467\n",
            "Epoch 148, Loss: 2.042335033416748\n",
            "Epoch 149, Loss: 1.9793033599853516\n",
            "Epoch 150, Loss: 1.9262734651565552\n",
            "Epoch 151, Loss: 1.946874976158142\n",
            "Epoch 152, Loss: 2.014610528945923\n",
            "Epoch 153, Loss: 2.0286524295806885\n",
            "Epoch 154, Loss: 2.0233206748962402\n",
            "Epoch 155, Loss: 2.0049078464508057\n",
            "Epoch 156, Loss: 1.9415398836135864\n",
            "Epoch 157, Loss: 1.9556316137313843\n",
            "Epoch 158, Loss: 1.974448323249817\n",
            "Epoch 159, Loss: 2.046347141265869\n",
            "Epoch 160, Loss: 2.0101559162139893\n",
            "Epoch 161, Loss: 1.9978125095367432\n",
            "Epoch 162, Loss: 1.972286343574524\n",
            "Epoch 163, Loss: 1.9732378721237183\n",
            "Epoch 164, Loss: 1.9673017263412476\n",
            "Epoch 165, Loss: 2.006319046020508\n",
            "Epoch 166, Loss: 2.0148696899414062\n",
            "Epoch 167, Loss: 1.989289402961731\n",
            "Epoch 168, Loss: 1.9796308279037476\n",
            "Epoch 169, Loss: 1.9847172498703003\n",
            "Epoch 170, Loss: 2.051335096359253\n",
            "Epoch 171, Loss: 2.0441720485687256\n",
            "Epoch 172, Loss: 2.039330005645752\n",
            "Epoch 173, Loss: 2.038785219192505\n",
            "Epoch 174, Loss: 2.0122578144073486\n",
            "Epoch 175, Loss: 1.978758454322815\n",
            "Epoch 176, Loss: 1.987922191619873\n",
            "Epoch 177, Loss: 2.0477094650268555\n",
            "Epoch 178, Loss: 2.0157110691070557\n",
            "Epoch 179, Loss: 2.0261905193328857\n",
            "Epoch 180, Loss: 2.012432098388672\n",
            "Epoch 181, Loss: 1.977789282798767\n",
            "Epoch 182, Loss: 2.0164055824279785\n",
            "Epoch 183, Loss: 2.037794589996338\n",
            "Epoch 184, Loss: 2.091949701309204\n",
            "Epoch 185, Loss: 2.014636754989624\n",
            "Epoch 186, Loss: 1.987610936164856\n",
            "Epoch 187, Loss: 2.0518734455108643\n",
            "Epoch 188, Loss: 2.0265982151031494\n",
            "Epoch 189, Loss: 1.9915030002593994\n",
            "Epoch 190, Loss: 1.956547498703003\n",
            "Epoch 191, Loss: 2.072106122970581\n",
            "Epoch 192, Loss: 1.9821513891220093\n",
            "Epoch 193, Loss: 1.9687294960021973\n",
            "Epoch 194, Loss: 1.9968013763427734\n",
            "Epoch 195, Loss: 2.066171884536743\n",
            "Epoch 196, Loss: 1.9540475606918335\n",
            "Epoch 197, Loss: 2.0838570594787598\n",
            "Epoch 198, Loss: 1.9713255167007446\n",
            "Epoch 199, Loss: 2.017456293106079\n",
            "Epoch 200, Loss: 1.9684264659881592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "_, pred = model(data).max(dim=1)\n",
        "correct = int(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
        "accuracy = correct / int(data.test_mask.sum())\n",
        "print(f'Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "id": "QdJyr0HqeGM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64796cf2-dcba-413e-b558-880ed5d0e1db"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JW863rWleGPw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}